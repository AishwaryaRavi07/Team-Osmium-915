{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to load the REBEL model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# wrapper for wikipedia API\n",
    "import wikipedia\n",
    "\n",
    "# scraping of web articles\n",
    "from newspaper import Article, ArticleException\n",
    "\n",
    "# google news scraping\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "# graph visualization\n",
    "from pyvis.network import Network\n",
    "\n",
    "# show HTML in notebook\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/Babelscape/rebel-large\n",
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge base class\n",
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.entities = {} # { entity_title: {...} }\n",
    "        self.relations = [] # [ head: entity_title, type: ..., tail: entity_title,\n",
    "          # meta: { article_url: { spans: [...] } } ]\n",
    "        self.sources = {} # { article_url: {...} }\n",
    "\n",
    "    def merge_relations(self, r2):\n",
    "        r1 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r2, r)][0]\n",
    "\n",
    "        # if different article\n",
    "        article_url = list(r2[\"meta\"].keys())[0]\n",
    "        if article_url not in r1[\"meta\"]:\n",
    "            r1[\"meta\"][article_url] = r2[\"meta\"][article_url]\n",
    "\n",
    "        # if existing article\n",
    "        else:\n",
    "            spans_to_add = [span for span in r2[\"meta\"][article_url][\"spans\"]\n",
    "                            if span not in r1[\"meta\"][article_url][\"spans\"]]\n",
    "            r1[\"meta\"][article_url][\"spans\"] += spans_to_add\n",
    "\n",
    "    def get_wikipedia_data(self, candidate_entity):\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary\n",
    "            }\n",
    "            return entity_data\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def add_entity(self, e):\n",
    "        self.entities[e[\"title\"]] = {k:v for k,v in e.items() if k != \"title\"}\n",
    "\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "\n",
    "    def merge_with_kb(self, kb2):\n",
    "        for r in kb2.relations:\n",
    "            article_url = list(r[\"meta\"].keys())[0]\n",
    "            source_data = kb2.sources[article_url]\n",
    "            self.add_relation(r, source_data[\"article_title\"],\n",
    "                              source_data[\"article_publish_date\"])\n",
    "\n",
    "    def add_relation(self, r, article_title, article_publish_date):\n",
    "        # check on wikipedia\n",
    "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
    "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
    "\n",
    "        # if one entity does not exist, stop\n",
    "        if any(ent is None for ent in entities):\n",
    "            return\n",
    "\n",
    "        # manage new entities\n",
    "        for e in entities:\n",
    "            self.add_entity(e)\n",
    "\n",
    "        # rename relation entities with their wikipedia titles\n",
    "        r[\"head\"] = entities[0][\"title\"]\n",
    "        r[\"tail\"] = entities[1][\"title\"]\n",
    "\n",
    "        # add source if not in kb\n",
    "        article_url = list(r[\"meta\"].keys())[0]\n",
    "        if article_url not in self.sources:\n",
    "            self.sources[article_url] = {\n",
    "                \"article_title\": article_title,\n",
    "                \"article_publish_date\": article_publish_date\n",
    "            }\n",
    "\n",
    "        # manage new relation\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for e in self.entities.items():\n",
    "            print(f\"  {e}\")\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "        print(\"Sources:\")\n",
    "        for s in self.sources.items():\n",
    "            print(f\"  {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relations for each span and put them together in a knowledge base\n",
    "def from_text_to_kb(text, article_url, span_length=128, article_title=None,\n",
    "                    article_publish_date=None, verbose=False):\n",
    "    # tokenize whole text\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "    # compute span boundaries\n",
    "    num_tokens = len(inputs[\"input_ids\"][0])\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_tokens} tokens\")\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_spans} spans\")\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) /\n",
    "                        max(num_spans - 1, 1))\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i,\n",
    "                                 start + span_length * (i + 1)])\n",
    "        start -= overlap\n",
    "    if verbose:\n",
    "        print(f\"Span boundaries are {spans_boundaries}\")\n",
    "\n",
    "    # transform input with spans\n",
    "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
    "                  for boundary in spans_boundaries]\n",
    "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
    "                    for boundary in spans_boundaries]\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "    }\n",
    "\n",
    "    # generate relations\n",
    "    num_return_sequences = 3\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    # decode relations\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
    "                                           skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    kb = KB()\n",
    "    i = 0\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = i // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for relation in relations:\n",
    "            relation[\"meta\"] = {\n",
    "                article_url: {\n",
    "                    \"spans\": [spans_boundaries[current_span_index]]\n",
    "                }\n",
    "            }\n",
    "            kb.add_relation(relation, article_title, article_publish_date)\n",
    "        i += 1\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse an article with newspaper3k\n",
    "def get_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article\n",
    "\n",
    "# extract the article from the url (along with metadata), extract relations and populate a KB\n",
    "def from_url_to_kb(url):\n",
    "    article = get_article(url)\n",
    "    config = {\n",
    "        \"article_title\": article.title,\n",
    "        \"article_publish_date\": article.publish_date\n",
    "    }\n",
    "    kb = from_text_to_kb(article.text, article.url, **config)\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get news links from google news\n",
    "def get_news_links(query, lang=\"en\", region=\"US\", pages=1, max_links=100000):\n",
    "    googlenews = GoogleNews(lang=lang, region=region)\n",
    "    googlenews.search(query)\n",
    "    all_urls = []\n",
    "    for page in range(pages):\n",
    "        googlenews.get_page(page)\n",
    "        all_urls += googlenews.get_links()\n",
    "    return list(set(all_urls))[:max_links]\n",
    "\n",
    "# build a KB from multiple news links\n",
    "def from_urls_to_kb(urls, verbose=False):\n",
    "    kb = KB()\n",
    "    if verbose:\n",
    "        print(f\"{len(urls)} links to visit\")\n",
    "    for url in urls:\n",
    "        if verbose:\n",
    "            print(f\"Visiting {url}...\")\n",
    "        try:\n",
    "            kb_url = from_url_to_kb(url)\n",
    "            kb.merge_with_kb(kb_url)\n",
    "        except ArticleException:\n",
    "            if verbose:\n",
    "                print(f\"  Couldn't download article at url {url}\")\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the `from_urls_to_kb` function\n",
    "news_links = get_news_links(\"Google\", pages=1, max_links=3)\n",
    "kb = from_urls_to_kb(news_links, verbose=True)\n",
    "kb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the `from_url_to_kb` function\n",
    "url = \"https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html\"\n",
    "kb = from_url_to_kb(url)\n",
    "kb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from KB to HTML visualization\n",
    "def save_network_html(kb, filename=\"network.html\"):\n",
    "    # create network\n",
    "    net = Network(directed=True, width=\"auto\", height=\"700px\", bgcolor=\"#eeeeee\")\n",
    "\n",
    "    # nodes\n",
    "    color_entity = \"#00FF00\"\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "    # edges\n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r[\"head\"], r[\"tail\"],\n",
    "                    title=r[\"type\"], label=r[\"type\"])\n",
    "\n",
    "    # save network\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract KB from news about Google and visualize it\n",
    "news_links = get_news_links(\"Google\", pages=5, max_links=20)\n",
    "kb = from_urls_to_kb(news_links, verbose=True)\n",
    "filename = \"network_3_google.html\"\n",
    "save_network_html(kb, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
