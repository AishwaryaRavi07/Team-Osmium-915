{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random \n",
    "import json \n",
    "import os \n",
    "import pickle \n",
    "import time \n",
    "import wikipedia as wp \n",
    "from wikipedia.exceptions import DisambiguationError,PageError\n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt \n",
    "import argparse\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "import nltk \n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch_geometric.utils.convert import from_networkx \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(topics=[\"tests\"], depth=20, max_size=20, simplify=False, plot=False, save_dir=None, max_nodes=None):\n",
    "    rg = RelationshipGenerator(save_dir=save_dir)\n",
    "\n",
    "    for topic in topics:\n",
    "        rg.scan(topic, max_nodes=max_nodes)\n",
    "\n",
    "    print(f\"Created {len(rg.links)} links with {rg.rank_terms().shape[0]} nodes.\")\n",
    "\n",
    "    links = rg.links\n",
    "    links = remove_self_references(links)\n",
    "\n",
    "    node_data = rg.rank_terms()\n",
    "    nodes = node_data.index.tolist()\n",
    "    node_weights = node_data.values.tolist()\n",
    "    node_weights = [nw * 100 for nw in node_weights]\n",
    "    nodelist = nodes\n",
    "\n",
    "\n",
    "    G = nx.DiGraph() # MultiGraph()\n",
    "\n",
    "    # G.add_node()\n",
    "    G.add_nodes_from(nodes)\n",
    "    feature_vectors, model = doc2vec(nodes, rg)\n",
    "    nx.set_node_attributes(G, feature_vectors, name=\"features\")\n",
    "\n",
    "    # Add edges\n",
    "    G.add_weighted_edges_from(links)\n",
    "    return G, nodelist, node_weights, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_references(l):\n",
    "    return[i for i in l if i[0]!=i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(nodes, rg):\n",
    "  # List of tuples page title, page content\n",
    "    features = dict(filter(lambda x: x[0] in nodes, rg.features.items()))\n",
    "    features = sorted(rg.features.items(), key=lambda key_value: nodes.index(key_value[0]))\n",
    "    tokenized_docs = [nltk.word_tokenize(' '.join(doc).lower()) for doc in features]\n",
    "    tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(tokenized_docs)]\n",
    "    # Model\n",
    "    model = Doc2Vec(vector_size=300, min_count=1, epochs=50)\n",
    "    model.build_vocab(tagged_docs)\n",
    "    model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    feature_vectors = {node: model.infer_vector(tokenized_docs[i]) for i, node in enumerate(nodes)}\n",
    "\n",
    "    return feature_vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationshipGenerator():\n",
    "    \"\"\"Generates relationships between terms, based on wikipedia links\"\"\"\n",
    "    def __init__(self, save_dir):\n",
    "        self.links = [] # [start, end, weight]\n",
    "        self.features = {} #{page: page_content}\n",
    "        self.page_links = {}\n",
    "\n",
    "\n",
    "    def scan(self, start=None, repeat=0, max_nodes=None):\n",
    "        print(\"On depth: \", repeat)\n",
    "        \"\"\"Start scanning from a specific word, or from internal database\n",
    "\n",
    "        Args:\n",
    "            start (str): the term to start searching from, can be None to let\n",
    "                algorithm decide where to start\n",
    "            repeat (int): the number of times to repeat the scan\n",
    "        \"\"\"\n",
    "        nodes_visited = 0\n",
    "        while repeat >= 0:\n",
    "            if max_nodes != None and nodes_visited == max_nodes:\n",
    "              return\n",
    "            # should check if start page exists\n",
    "            # and haven't already scanned\n",
    "            # if start in [l[0] for l in self.links]:\n",
    "            #     raise Exception(\"Already scanned\")\n",
    "\n",
    "            term_search = True if start is not None else False\n",
    "\n",
    "            # If a start isn't defined, we should find one\n",
    "            if start is None:\n",
    "                start = self.find_starting_point()\n",
    "\n",
    "            # Scan the starting point specified for links\n",
    "            print(f\"Scanning page {start}...\")\n",
    "            try:\n",
    "                # Fetch the page through the Wikipedia API\n",
    "                page = wp.page(start)\n",
    "                self.features[start] = page.content\n",
    "                links = list(set(page.links))\n",
    "\n",
    "                # ignore some uninteresting terms\n",
    "                links = [l for l in links if not self.ignore_term(l)]\n",
    "\n",
    "                # Add links to database\n",
    "                link_weights = []\n",
    "                for link in links:\n",
    "                    weight = self.weight_link(page, link)\n",
    "                    link_weights.append(weight)\n",
    "\n",
    "                link_weights = [w / max(link_weights) for w in link_weights]\n",
    "\n",
    "                #add the links\n",
    "                for i, link in enumerate(links):\n",
    "                  if max_nodes != None and nodes_visited == max_nodes:\n",
    "                    return\n",
    "\n",
    "                 #Access all the pages that link to the links that have been added\n",
    "                  try:\n",
    "                    link = link.lower()\n",
    "                    if link not in self.features or link not in self.page_links:\n",
    "                        time.sleep(np.random.randint(0, 10))\n",
    "                        page = wp.page(link)\n",
    "                        self.features[link] = page.content\n",
    "                        self.page_links[link] = [l.lower() for l in page.links]\n",
    "                        print(\"Page Accessed: \", link)\n",
    "                        nodes_visited += 1\n",
    "                    else:\n",
    "                        print(\"Page has previously been accessed: \", link)\n",
    "                    total_nodes = set([l[1].lower() for l in self.links])\n",
    "                    for links_to in set([l.lower() for l in self.page_links[link]]).intersection(total_nodes):\n",
    "                        self.links.append([link, links_to, 0.1]) # 3 works pretty well\n",
    "                        print(\"hi\")\n",
    "                    self.links.append([start, link, link_weights[i] + 2 * int(term_search)]) # 3 works pretty well\n",
    "\n",
    "                  except (DisambiguationError, PageError):\n",
    "                      print(\"Page not found: \", link)\n",
    "\n",
    "\n",
    "                # Print some data to the user on progress\n",
    "                explored_nodes = set([l[0] for l in self.links])\n",
    "                explored_nodes_count = len(explored_nodes)\n",
    "                total_nodes = set([l[1] for l in self.links])\n",
    "                total_nodes_count = len(total_nodes)\n",
    "                new_nodes = [l.lower() for l in links if l not in total_nodes]\n",
    "                new_nodes_count = len(new_nodes)\n",
    "                print(f\"New nodes added: {new_nodes_count}, Total Nodes: {total_nodes_count}, Explored Nodes: {explored_nodes_count}\")\n",
    "\n",
    "            except (DisambiguationError, PageError):\n",
    "                # This happens if the page has disambiguation or doesn't exist\n",
    "                # We just ignore the page for now, could improve this\n",
    "                # self.links.append([start, \"DISAMBIGUATION\", 0])\n",
    "                print(\"ERROR, I DID NOT GET THIS PAGE\")\n",
    "                pass\n",
    "\n",
    "            repeat -= 1\n",
    "            start = None\n",
    "\n",
    "    def find_starting_point(self):\n",
    "        \"\"\"Find the best place to start when no input is given\"\"\"\n",
    "        # Need some links to work with.\n",
    "        if len(self.links) == 0:\n",
    "            raise Exception(\"Unable to start, no start defined or existing links\")\n",
    "\n",
    "        # Get top terms\n",
    "        res = self.rank_terms()\n",
    "        sorted_links = list(zip(res.index, res.values))\n",
    "        all_starts = set([l[0] for l in self.links])\n",
    "\n",
    "        # Remove identifiers (these are on many Wikipedia pages)\n",
    "        all_starts = [l for l in all_starts if '(identifier)' not in l]\n",
    "\n",
    "        # print(sorted_links[:10])\n",
    "        # Iterate over the top links, until we find a new one\n",
    "        for i in range(len(sorted_links)):\n",
    "            if sorted_links[i][0] not in all_starts and len(sorted_links[i][0]) > 0:\n",
    "                return sorted_links[i][0]\n",
    "\n",
    "        # no link found\n",
    "        raise Exception(\"No starting point found within links\")\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_link(page, link):\n",
    "        \"\"\"Weight an outgoing link for a given source page\n",
    "\n",
    "        Args:\n",
    "            page (obj):\n",
    "            link (str): the outgoing link of interest\n",
    "\n",
    "        Returns:\n",
    "            (float): the weight, between 0 and 1\n",
    "        \"\"\"\n",
    "        weight = 0.1\n",
    "\n",
    "        link_counts = page.content.lower().count(link.lower())\n",
    "        weight += link_counts\n",
    "\n",
    "        if link.lower() in page.summary.lower():\n",
    "            weight += 3\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def rank_terms(self, with_start=True):\n",
    "        # We can use graph theory here!\n",
    "        # tws = [l[1:] for l in self.links]\n",
    "        df = pd.DataFrame(self.links, columns=[\"start\", \"end\", \"weight\"])\n",
    "\n",
    "        if with_start:\n",
    "            df = df.append(df.rename(columns={\"end\": \"start\", \"start\":\"end\"}))\n",
    "\n",
    "        return df.groupby(\"end\").weight.sum().sort_values(ascending=False)\n",
    "\n",
    "    def get_key_terms(self, n=20):\n",
    "        return \"'\" + \"', '\".join([t for t in self.rank_terms().head(n).index.tolist() if \"(identifier)\" not in t]) + \"'\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ignore_term(term):\n",
    "        \"\"\"List of terms to ignore\"\"\"\n",
    "        if \"(identifier)\" in term or term == \"doi\":\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, nodelist, node_weights, model = create_graph(topics=[\"z-test\",\"standard deviation\",\"t-test\",\"mean\"], max_nodes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_plot(G, nodelist, node_weights):\n",
    "    pos = nx.spring_layout(G, k=1, seed=7)  # positions for all nodes - seed for reproducibility\n",
    "\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        nodelist=nodelist,\n",
    "        node_size=node_weights,\n",
    "        node_color='lightblue',\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    widths = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        edgelist = widths.keys(),\n",
    "        width=list(widths.values()),\n",
    "        edge_color='lightblue',\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos=pos,\n",
    "                            labels=dict(zip(nodelist,nodelist)),\n",
    "                            font_color='black')\n",
    "    fig = plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_plot(G, nodelist, node_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
